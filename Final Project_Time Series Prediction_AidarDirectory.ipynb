{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Time Series Prediction by LSTM"],"metadata":{"id":"Cwnhdanjxhvx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"L_PlBMr6xG99"},"outputs":[],"source":["# ignores all warnings and prevents them from being printed to the console.\n","import warnings\n","warnings.filterwarnings('ignore')\n","path_prefix = './'"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hUxyWRXxPDNL","executionInfo":{"status":"ok","timestamp":1681071668413,"user_tz":-480,"elapsed":23494,"user":{"displayName":"Aidar Garifullin","userId":"09296219436634757120"}},"outputId":"efafdb97-3078-47e7-b6d6-4b121ca85a8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## 1. Data loader"],"metadata":{"id":"b73Q3oqg35QE"}},{"cell_type":"code","source":["import math\n","import numpy as np\n","import pandas as pd\n","\n","class DataLoader():\n","    \"\"\"A class for loading and transforming data for the lstm model\"\"\"\n","\n","    def __init__(self, filename, split, cols):\n","        dataframe = pd.read_csv(filename)\n","        i_split = int(len(dataframe) * split)\n","        self.data_train = dataframe.get(cols).values[:i_split]\n","        self.data_test  = dataframe.get(cols).values[i_split:]\n","        self.len_train  = len(self.data_train)\n","        self.len_test   = len(self.data_test)\n","        self.len_train_windows = None\n","\n","    def get_test_data(self, seq_len, normalise):\n","        '''\n","        Create x, y test data windows\n","        Warning: batch method, not generative, make sure you have enough memory to\n","        load data, otherwise reduce size of the training split.\n","        '''\n","        data_windows = []\n","        for i in range(self.len_test - seq_len):\n","            data_windows.append(self.data_test[i:i+seq_len])\n","\n","        data_windows = np.array(data_windows).astype(float)\n","        data_windows = self.normalise_windows(data_windows, single_window=False) if normalise else data_windows\n","\n","        x = data_windows[:, :-1]\n","        y = data_windows[:, -1, [0]]\n","        return x,y\n","\n","    def get_train_data(self, seq_len, normalise):\n","        '''\n","        Create x, y train data windows\n","        Warning: batch method, not generative, make sure you have enough memory to\n","        load data, otherwise use generate_training_window() method.\n","        '''\n","        data_x = []\n","        data_y = []\n","        for i in range(self.len_train - seq_len):\n","            x, y = self._next_window(i, seq_len, normalise)\n","            data_x.append(x)\n","            data_y.append(y)\n","        return np.array(data_x), np.array(data_y)\n","\n","    def generate_train_batch(self, seq_len, batch_size, normalise):\n","        '''Yield a generator of training data from filename on given list of cols split for train/test'''\n","        i = 0\n","        while i < (self.len_train - seq_len):\n","            x_batch = []\n","            y_batch = []\n","            for b in range(batch_size):\n","                if i >= (self.len_train - seq_len):\n","                    # stop-condition for a smaller final batch if data doesn't divide evenly\n","                    yield np.array(x_batch), np.array(y_batch)\n","                    i = 0\n","                x, y = self._next_window(i, seq_len, normalise)\n","                x_batch.append(x)\n","                y_batch.append(y)\n","                i += 1\n","            yield np.array(x_batch), np.array(y_batch)\n","\n","    def _next_window(self, i, seq_len, normalise):\n","        '''Generates the next data window from the given index location i'''\n","        window = self.data_train[i:i+seq_len]\n","        window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n","        x = window[:-1]\n","        y = window[-1, [0]]\n","        return x, y\n","\n","    def normalise_windows(self, window_data, single_window=False):\n","        '''Normalise window with a base value of zero'''\n","        normalised_data = []\n","        window_data = [window_data] if single_window else window_data\n","        for window in window_data:\n","            normalised_window = []\n","            for col_i in range(window.shape[1]):\n","                normalised_col = [((float(p) / float(window[0, col_i])) - 1) for p in window[:, col_i]]\n","                normalised_window.append(normalised_col)\n","            normalised_window = np.array(normalised_window).T # reshape and transpose array back into original multidimensional format\n","            normalised_data.append(normalised_window)\n","        return np.array(normalised_data)"],"metadata":{"id":"ENAKdPXIxqLC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import datetime as dt\n","from numpy import newaxis\n","# from core.utils import Timer\n","from keras.layers import Dense, Activation, Dropout, LSTM\n","from keras.models import Sequential, load_model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","class Model():\n","\t\"\"\"A class for an building and inferencing an lstm model\"\"\"\n","\n","\tdef __init__(self):\n","\t\tself.model = Sequential()\n","\n","\tdef load_model(self, filepath):\n","\t\tprint('[Model] Loading model from file %s' % filepath)\n","\t\tself.model = load_model(filepath)\n","\n","\tdef build_model(self, configs):\n","\t\t# timer = Timer()\n","\t\t# timer.start()\n","\n","\t\tfor layer in configs['model']['layers']:\n","\t\t\tneurons = layer['neurons'] if 'neurons' in layer else None\n","\t\t\tdropout_rate = layer['rate'] if 'rate' in layer else None\n","\t\t\tactivation = layer['activation'] if 'activation' in layer else None\n","\t\t\treturn_seq = layer['return_seq'] if 'return_seq' in layer else None\n","\t\t\tinput_timesteps = layer['input_timesteps'] if 'input_timesteps' in layer else None\n","\t\t\tinput_dim = layer['input_dim'] if 'input_dim' in layer else None\n","\n","\t\t\tif layer['type'] == 'dense':\n","\t\t\t\tself.model.add(Dense(neurons, activation=activation))\n","\t\t\tif layer['type'] == 'lstm':\n","\t\t\t\tself.model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=return_seq))\n","\t\t\tif layer['type'] == 'dropout':\n","\t\t\t\tself.model.add(Dropout(dropout_rate))\n","\n","\t\tself.model.compile(loss=configs['model']['loss'], optimizer=configs['model']['optimizer'])\n","\n","\t\tprint('[Model] Model Compiled')\n","\t\t# timer.stop()\n","\n","\tdef train(self, x, y, epochs, batch_size, save_dir):\n","\t\t# timer = Timer()\n","\t\t# timer.start()\n","\t\tprint('[Model] Training Started')\n","\t\tprint('[Model] %s epochs, %s batch size' % (epochs, batch_size))\n","\t\t\n","\t\tsave_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\n","\t\tcallbacks = [\n","\t\t\tEarlyStopping(monitor='val_loss', patience=2),\n","\t\t\tModelCheckpoint(filepath=save_fname, monitor='val_loss', save_best_only=True)\n","\t\t]\n","\t\tself.model.fit(\n","\t\t\tx,\n","\t\t\ty,\n","\t\t\tepochs=epochs,\n","\t\t\tbatch_size=batch_size,\n","\t\t\tcallbacks=callbacks\n","\t\t)\n","\t\tself.model.save(save_fname)\n","\n","\t\tprint('[Model] Training Completed. Model saved as %s' % save_fname)\n","\t\t# timer.stop()\n","\n","\tdef train_generator(self, data_gen, epochs, batch_size, steps_per_epoch, save_dir):\n","\t\t# timer = Timer()\n","\t\t# timer.start()\n","\t\tprint('[Model] Training Started')\n","\t\tprint('[Model] %s epochs, %s batch size, %s batches per epoch' % (epochs, batch_size, steps_per_epoch))\n","\t\t\n","\t\tsave_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\n","\t\tcallbacks = [\n","\t\t\tModelCheckpoint(filepath=save_fname, monitor='loss', save_best_only=True)\n","\t\t]\n","\t\tself.model.fit_generator(\n","\t\t\tdata_gen,\n","\t\t\tsteps_per_epoch=steps_per_epoch,\n","\t\t\tepochs=epochs,\n","\t\t\tcallbacks=callbacks,\n","\t\t\tworkers=1\n","\t\t)\n","\t\t\n","\t\tprint('[Model] Training Completed. Model saved as %s' % save_fname)\n","\t\t# timer.stop()\n","\n","\tdef predict_point_by_point(self, data):\n","\t\t#Predict each timestep given the last sequence of true data, in effect only predicting 1 step ahead each time\n","\t\tprint('[Model] Predicting Point-by-Point...')\n","\t\tpredicted = self.model.predict(data)\n","\t\tpredicted = np.reshape(predicted, (predicted.size,))\n","\t\treturn predicted\n","\n","\tdef predict_sequences_multiple(self, data, window_size, prediction_len):\n","\t\t#Predict sequence of 50 steps before shifting prediction run forward by 50 steps\n","\t\tprint('[Model] Predicting Sequences Multiple...')\n","\t\tprediction_seqs = []\n","\t\tfor i in range(int(len(data)/prediction_len)):\n","\t\t\tcurr_frame = data[i*prediction_len]\n","\t\t\tpredicted = []\n","\t\t\tfor j in range(prediction_len):\n","\t\t\t\tpredicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n","\t\t\t\tcurr_frame = curr_frame[1:]\n","\t\t\t\tcurr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n","\t\t\tprediction_seqs.append(predicted)\n","\t\treturn prediction_seqs\n","\n","\tdef predict_sequence_full(self, data, window_size):\n","\t\t#Shift the window by 1 new prediction each time, re-run predictions on new window\n","\t\tprint('[Model] Predicting Sequences Full...')\n","\t\tcurr_frame = data[0]\n","\t\tpredicted = []\n","\t\tfor i in range(len(data)):\n","\t\t\tpredicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n","\t\t\tcurr_frame = curr_frame[1:]\n","\t\t\tcurr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n","\t\treturn predicted"],"metadata":{"id":"5LI2jyc9ydI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datetime as dt\n","\n","class Timer():\n","\n","\tdef __init__(self):\n","\t\tself.start_dt = None\n","\n","\tdef start(self):\n","\t\tself.start_dt = dt.datetime.now()\n","\n","\tdef stop(self):\n","\t\tend_dt = dt.datetime.now()\n","\t\tprint('Time taken: %s' % (end_dt - self.start_dt))"],"metadata":{"id":"LqZDmaTMyy7V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import time\n","import matplotlib.pyplot as plt\n","# from core.data_processor import DataLoader\n","# from core.model import Model\n","\n","def plot_results(predicted_data, true_data):\n","    fig = plt.figure(facecolor='white')\n","    ax = fig.add_subplot(111)\n","    ax.plot(true_data, label='True Data')\n","    plt.plot(predicted_data, label='Prediction')\n","    plt.legend()\n","    plt.show()\n","\n","def plot_results_multiple(predicted_data, true_data, prediction_len):\n","    fig = plt.figure(facecolor='white')\n","    ax = fig.add_subplot(111)\n","    ax.plot(true_data, label='True Data')\n","\t# Pad the list of predictions to shift it in the graph to it's correct start\n","    for i, data in enumerate(predicted_data):\n","        padding = [None for p in range(i * prediction_len)]\n","        plt.plot(padding + data, label='Prediction')\n","        plt.legend()\n","    plt.show()"],"metadata":{"id":"v_nOKSi102qK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    # Load config file\n","    import json\n","    with open('/content/drive/MyDrive/config.json', 'r') as f:\n","    #with open('/content/drive/MyDrive/CityU/ Courses/3. Junior/Semester B/SDSC4016_ML2/SDSC4016_Group Project/Project/config.json', 'r') as f:\n","      configs = json.load(f)\n","    # configs = json.load(open('config.json', 'r'))\n","    if not os.path.exists(configs['model']['save_dir']): os.makedirs(configs['model']['save_dir'])\n","\n","    data = DataLoader(\n","        os.path.join('data', configs['data']['filename']),\n","        configs['data']['train_test_split'],\n","        configs['data']['columns']\n","    )\n","\n","    model = Model()\n","    model.build_model(configs)\n","    x, y = data.get_train_data(\n","        seq_len=configs['data']['sequence_length'],\n","        normalise=configs['data']['normalise']\n","    )\n","\n","    '''\n","\t# in-memory training\n","\tmodel.train(\n","\t\tx,\n","\t\ty,\n","\t\tepochs = configs['training']['epochs'],\n","\t\tbatch_size = configs['training']['batch_size'],\n","\t\tsave_dir = configs['model']['save_dir']\n","\t)\n","\t'''\n","    # out-of memory generative training\n","    steps_per_epoch = math.ceil((data.len_train - configs['data']['sequence_length']) / configs['training']['batch_size'])\n","    model.train_generator(\n","        data_gen=data.generate_train_batch(\n","            seq_len=configs['data']['sequence_length'],\n","            batch_size=configs['training']['batch_size'],\n","            normalise=configs['data']['normalise']\n","        ),\n","        epochs=configs['training']['epochs'],\n","        batch_size=configs['training']['batch_size'],\n","        steps_per_epoch=steps_per_epoch,\n","        save_dir=configs['model']['save_dir']\n","    )\n","\n","    x_test, y_test = data.get_test_data(\n","        seq_len=configs['data']['sequence_length'],\n","        normalise=configs['data']['normalise']\n","    )\n","\n","    predictions = model.predict_sequences_multiple(x_test, configs['data']['sequence_length'], configs['data']['sequence_length'])\n","    # predictions = model.predict_sequence_full(x_test, configs['data']['sequence_length'])\n","    # predictions = model.predict_point_by_point(x_test)\n","\n","    plot_results_multiple(predictions, y_test, configs['data']['sequence_length'])\n","    # plot_results(predictions, y_test)\n","\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"PYVpbDhmW9el","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1681072308131,"user_tz":-480,"elapsed":14405,"user":{"displayName":"Aidar Garifullin","userId":"09296219436634757120"}},"outputId":"8301de7c-3a8a-4db5-fe62-8232681073e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Model] Model Compiled\n","[Model] Training Started\n","[Model] 2 epochs, 32 batch size, 3881 batches per epoch\n","Epoch 1/2\n"]},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-a9a91de830a5>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-a9a91de830a5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# out-of memory generative training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen_train\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sequence_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     model.train_generator(\n\u001b[0m\u001b[1;32m     36\u001b[0m         data_gen=data.generate_train_batch(\n\u001b[1;32m     37\u001b[0m             \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sequence_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-0913fdec8f77>\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(self, data_gen, epochs, batch_size, steps_per_epoch, save_dir)\u001b[0m\n\u001b[1;32m     74\u001b[0m                         \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \t\t]\n\u001b[0;32m---> 76\u001b[0;31m \t\tself.model.fit_generator(\n\u001b[0m\u001b[1;32m     77\u001b[0m                         \u001b[0mdata_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2634\u001b[0m             \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m         )\n\u001b[0;32m-> 2636\u001b[0;31m         return self.fit(\n\u001b[0m\u001b[1;32m   2637\u001b[0m             \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'while/MatMul' defined at (most recent call last):\n    File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-8-a9a91de830a5>\", line 61, in <cell line: 60>\n      main()\n    File \"<ipython-input-8-a9a91de830a5>\", line 35, in main\n      model.train_generator(\n    File \"<ipython-input-4-0913fdec8f77>\", line 76, in train_generator\n      self.model.fit_generator(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 2636, in fit_generator\n      return self.fit(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/base_rnn.py\", line 556, in __call__\n      return super().__call__(inputs, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/lstm.py\", line 748, in call\n      ) = lstm_with_backend_selection(**normal_lstm_kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/lstm.py\", line 1338, in lstm_with_backend_selection\n      last_output, outputs, new_h, new_c, runtime = defun_standard_lstm(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/lstm.py\", line 980, in standard_lstm\n      last_output, outputs, new_states = backend.rnn(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/backend.py\", line 5169, in rnn\n      final_outputs = tf.compat.v1.while_loop(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/backend.py\", line 5148, in _step\n      output, new_states = step_function(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/lstm.py\", line 966, in step\n      z = backend.dot(cell_inputs, kernel)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/backend.py\", line 2464, in dot\n      out = tf.matmul(x, y)\nNode: 'while/MatMul'\nMatrix size-incompatible: In[0]: [32,1], In[1]: [2,400]\n\t [[{{node while/MatMul}}]]\n\t [[sequential/lstm/PartitionedCall]] [Op:__inference_train_function_8605]"]}]}]}